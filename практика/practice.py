import requests
from bs4 import BeautifulSoup
import csv
from request_test import get_image

def get_html(url):
    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    with open('output.html', 'w', encoding='utf-8') as f:
        f.write(r.text)
    return r.text

def get_href(url):
    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    with open('Hrefoutput.html', 'w', encoding='utf-8') as f:
        f.write(r.text)
    return r.text


def new_write_csv():
    with open('events.csv', 'w', encoding='utf-8', newline='') as f:  # 'w' –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
        writer = csv.writer(f)
        writer.writerow(['–ù–∞–∑–≤–∞–Ω–∏–µ', '–¶–µ–Ω–∞ (‚Ç∏)', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü–∞—Ä—Ç–Ω—ë—Ä', '–î–∞—Ç–∞', "–ì–æ—Ä–æ–¥"])

def write_csv(data):
    with open('href.csv', 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerows(data)  # –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —Å–ø–∏—Å–æ–∫ –∏–∑ 3 —ç–ª–µ–º–µ–Ω—Ç–æ–≤

def write_csv_row(row):
    with open('events.csv', 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([row[0], row[1], row[2], row[3], row[4], row[5]])

def get_href_data(html, slug, src : str):
    soup = BeautifulSoup(html, "lxml")
    data = []
    
    try:
        main_div = soup.find("div", class_="container")
        if main_div is None:
            return "No container found"
        
        # Get attributes with safe fallbacks
        title = str(main_div.get("data-title", "–Ω–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è"))
        price = str(main_div.get("data-minprice", "not price"))
        partner = str(main_div.get("data-partner", "–Ω–µ—Ç partner"))
        category = str(main_div.get("data-category", "–Ω–µ—Ç category"))
        
        add_div = main_div.find('div', class_="event_date_block")
        date = str(add_div.get("data-date", 'No data')) if add_div else 'No data'
        
        data = [title, price, category, partner, date, slug]
        write_csv_row(data)
        get_image(src, title)
        return f"{title} | {price} ‚Ç∏ | {category} | {partner} | {date} | {slug}"
    except Exception as e:
        print(f"Error processing data: {str(e)}")
        return "Error processing data"


def get_data(html, slug):
    soup = BeautifulSoup(html, "lxml")
    data = []

    cards = soup.find_all("div", class_="impression-card")
    
    # üîç –ï—Å–ª–∏ –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–µ—Ç ‚Äî —ç—Ç–æ –∫–æ–Ω–µ—Ü
    if not cards:
        return None

    for card in cards:
        a_tag = card.find("a")
        img = a_tag.find("img")
        src = img.get("src")
        href = a_tag.get("href")
        if href:
            data_page = get_href(href)
            need_data = get_href_data(data_page, slug, src)
            data.append(need_data)

    return data




def main(city: str, slug):
    page = 1

    while True:
        url = f'{city}afisha?page={page}'
        print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        html = get_html(url)
        parsed = get_data(html, slug)

        if parsed is None:  # üëâ –µ—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ä—Ç–æ—á–µ–∫, –≤—ã—Ö–æ–¥–∏–º
            print("‚ùå –ë–æ–ª—å—à–µ –Ω–µ—Ç –∞—Ñ–∏—à–∏, –≤—ã—Ö–æ–¥–∏–º.")
            break

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(parsed)}")
        page += 1

if __name__ == '__main__':
    main()
